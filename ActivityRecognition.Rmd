---
title: "Human Activity Recognition Report"
author: "Ezra Tucker"
date: "May 12, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

## Activity Recognition Report

### Introduction
The Weight Lifting Exercises Dataset available on [groupware](http://groupware.les.inf.puc-rio.br/har) has a description for dumbbell exercises done correctly (`classe = A`) and done incorrectly with common mistakes (`classe = B,C,D,E`). Alongside these determinations is a significant amount of sensor data, taken from a belt, the subjects' forearm, and on the dumbbell itself. The goal of this project is to predict the result of some dumbbell exercises, (ie, predict the `classe` variable) given new sensor data in the testing data set.

The method that we will use here is many random forests, which will be combined together in an ensemble.

### Downloading data
```{r data, echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE}
library(caret)
set.seed(1230)
testingFPath <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainingFPath <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing <- read.csv(testingFPath)
fullset <- read.csv(trainingFPath)
inTrain = createDataPartition(fullset$classe, p = 3/4)[[1]]
training = fullset[inTrain,]
validation = fullset[-inTrain,]
```
We download the data directly so as to avoid read/write access issues in replication.

### Models
The strategy employed here to create different models is grouping by sensor and measurement. The archetype for these models is as follows:

```{r printModels, eval = FALSE}
set.seed(1234)
models <- list()
models$rpy_belt <- train(classe ~ roll_belt + pitch_belt + yaw_belt,
                         data = training, method = "rf")
# etc
```

```{r models, cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(1234)

models <- list()
models$rpy_belt <- train(classe ~ roll_belt + pitch_belt + yaw_belt,
                         data = training, method = "rf")
models$rpy_dumbbell <- train(classe ~ roll_dumbbell + pitch_dumbbell + yaw_dumbbell,
                             data = training, method = "rf")
models$rpy_forearm <- train(classe ~ roll_forearm + pitch_forearm + yaw_forearm,
                            data = training, method = "rf")
models$gyros_belt <- train(classe ~ gyros_belt_x + gyros_belt_y + gyros_belt_z,
                           data = training, method = "rf")
models$accel_belt <- train(classe ~ accel_belt_x + accel_belt_y + accel_belt_z,
                           data = training, method = "rf")
models$magnet_belt <- train(classe ~ magnet_belt_x + magnet_belt_y + magnet_belt_z,
                            data = training, method = "rf")
models$gyros_dumbbell <- train(classe ~ gyros_dumbbell_x + gyros_dumbbell_y + gyros_dumbbell_z,
                               data = training, method = "rf")
models$accel_dumbbell <- train(classe ~ accel_dumbbell_x + accel_dumbbell_y + accel_dumbbell_z,
                               data = training, method = "rf")
models$magnet_dumbbell <- train(classe ~ magnet_dumbbell_x + magnet_dumbbell_y + magnet_dumbbell_z,
                                data = training, method = "rf")
models$gyros_forearm <- train(classe ~ gyros_forearm_x + gyros_forearm_y + gyros_forearm_z,
                              data = training, method = "rf")
models$accel_forearm <- train(classe ~ accel_forearm_x + accel_forearm_y + accel_forearm_z,
                              data = training, method = "rf")
models$magnet_forearm <- train(classe ~ magnet_forearm_x + magnet_forearm_y + magnet_forearm_z,
                               data = training, method = "rf")
```


Please see `.Rmd` file for complete listing of run models.

The `models` list contains each of the models, each follows the same pattern. They are all predicting the `classe` variable, are all random forests, run off of the `training` set, the only differences are of the input variables. The `models` list also contains data from the gyro sensor, the accelerometer, and magnet sensor for each of the belt, the forearm, and the dumbbell sensors, generating twelve independent predictive models.

The accuracies (in-sample) of each of these models are as follows:
```{r accuracies, echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE}
library(dplyr, warn.conflicts = FALSE); library(knitr); library(caret)
model_names = sapply(1:12, function(i) deparse(substitute(models[[i]])))
resultDf <- do.call(rbind, lapply(1:length(models), function(i) {
    postResample(predict(models[[i]], training), training$classe)
})) %>% as.data.frame %>%
  mutate(model_name = names(models)) %>%
  select(model_name, Accuracy, Kappa)
knitr::kable(resultDf)
```

The accuracies for some of the above models seem high, and for that reason we'll use all of the above, to try to avoid over-fitting.

The last step here is to combine the results given by the predictions in the `models` list. This could be done in a more sophisticated way; however, it should suffice for our purposes to simply have each row "vote", ie, take the mode of each row. For that, we'll want to define a `modeFunc`:

```{r voting}
modeFunc <- function(x) {
  uniqueX <- unique(x)
  uniqueX[which.max(tabulate(match(x, uniqueX)))]
}

predictionDf <- lapply(models, function(m) predict(m, training)) %>% as.data.frame
predTraining <- apply(predictionDf, 1, modeFunc)
```

### Performance and Error

We will want to assess the in-sample error, which was 75% of the training set available from the data source, using a `confusionMatrix`. We can get the out-of-sample error estimate by using the remainder, earlier referred to as the `validation` set.

```{r inAndOutErrors, cache = FALSE, echo = TRUE}
#In-sample confusion matrix
print(confusionMatrix(predTraining, training$classe))

#Out-of-sample confusion matrix
predValidation <- lapply(models, function(m) predict(m, validation)) %>%
  as.data.frame %>%
  apply(1, modeFunc)
print(confusionMatrix(predValidation, reference = validation$classe))
```

The in-sample error (1 - accuracy) here is 0%, whereas the out-of-sample error is about 11%; though high, is acceptable for this exercise.

### Prediction

What remains is to predict the values in the testing data set.

```{r testingPrediction, cache = TRUE, echo = TRUE, }
predFinal <- lapply(models, function(m) predict(m, testing)) %>%
  as.data.frame %>%
  apply(1, modeFunc)
predFinalDf <- data.frame(observation = 1:length(predFinal), classe = predFinal)
knitr::kable(predFinalDf)
```

### Discussion
There hasn't been a huge effort towards variable selection. It was generally assumed that the variables chosen here yield acceptable models, an assumption that worked out well (with the possible exceptions of `gyros_belt` and `accel_belt`). The out-of-sample error rate is quite high, and more care could be taken with variable selection.

There may also be gains to be had with combining various different types of models, which was not attempted here.

It should be also noted here that the simple voting model, while chosen for its speed, may not have even been the best performing model.